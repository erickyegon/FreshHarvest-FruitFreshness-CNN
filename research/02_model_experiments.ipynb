{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreshHarvest Model Experiments\n",
    "\n",
    "This notebook contains experiments with different CNN architectures for fruit freshness classification.\n",
    "We'll compare Basic CNN, Improved CNN with ResNet blocks, and Lightweight CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Import custom modules\n",
    "from cvProject_FreshHarvest.utils.common import read_yaml, setup_logging\n",
    "from cvProject_FreshHarvest.models.cnn_models import FreshHarvestCNN\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = read_yaml('../config/config.yaml')\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Image size: {config['data']['image_size']}\")\n",
    "print(f\"- Number of classes: {config['data']['num_classes']}\")\n",
    "print(f\"- Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"- Learning rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "def create_data_generators(batch_size=32):\n",
    "    \"\"\"Create training and validation data generators.\"\"\"\n",
    "    \n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation data generator (no augmentation)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        '../data/processed/train',\n",
    "        target_size=tuple(config['data']['image_size']),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        '../data/processed/val',\n",
    "        target_size=tuple(config['data']['image_size']),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Create generators\n",
    "train_gen, val_gen = create_data_generators()\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")\n",
    "print(f\"Classes: {list(train_gen.class_indices.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model builder\n",
    "cnn_builder = FreshHarvestCNN('../config/config.yaml')\n",
    "\n",
    "# Create different model architectures\n",
    "models = {\n",
    "    'Basic CNN': cnn_builder.create_basic_cnn(),\n",
    "    'Improved CNN': cnn_builder.create_improved_cnn(),\n",
    "    'Lightweight CNN': cnn_builder.create_lightweight_cnn()\n",
    "}\n",
    "\n",
    "# Display model summaries\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total parameters: {model.count_params():,}\")\n",
    "    print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "    \n",
    "    # Show model architecture\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_experiment(model, model_name, epochs=10):\n",
    "    \"\"\"Train a model and return training history.\"\"\"\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=config['training']['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        steps_per_epoch=min(50, train_gen.samples // config['training']['batch_size']),\n",
    "        validation_steps=min(20, val_gen.samples // config['training']['batch_size'])\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train all models (short training for comparison)\n",
    "training_histories = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        history = train_model_experiment(model, name, epochs=5)\n",
    "        training_histories[name] = history\n",
    "        print(f\"‚úÖ {name} training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name} training failed: {e}\")\n",
    "        training_histories[name] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "def plot_training_comparison(histories):\n",
    "    \"\"\"Plot training metrics comparison across models.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['accuracy', 'loss', 'precision', 'recall']\n",
    "    titles = ['Training Accuracy', 'Training Loss', 'Training Precision', 'Training Recall']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        for name, history in histories.items():\n",
    "            if history is not None:\n",
    "                if metric in history.history:\n",
    "                    ax.plot(history.history[metric], label=f'{name} - Train', marker='o')\n",
    "                if f'val_{metric}' in history.history:\n",
    "                    ax.plot(history.history[f'val_{metric}'], label=f'{name} - Val', marker='s', linestyle='--')\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison if we have training histories\n",
    "if any(h is not None for h in training_histories.values()):\n",
    "    plot_training_comparison(training_histories)\n",
    "else:\n",
    "    print(\"No successful training histories to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "def create_performance_summary(models, histories):\n",
    "    \"\"\"Create a summary table of model performance.\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        history = histories.get(name)\n",
    "        \n",
    "        # Model complexity metrics\n",
    "        total_params = model.count_params()\n",
    "        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "        \n",
    "        # Performance metrics (if training was successful)\n",
    "        if history is not None:\n",
    "            final_train_acc = history.history['accuracy'][-1] if 'accuracy' in history.history else 0\n",
    "            final_val_acc = history.history['val_accuracy'][-1] if 'val_accuracy' in history.history else 0\n",
    "            final_train_loss = history.history['loss'][-1] if 'loss' in history.history else float('inf')\n",
    "            final_val_loss = history.history['val_loss'][-1] if 'val_loss' in history.history else float('inf')\n",
    "            best_val_acc = max(history.history['val_accuracy']) if 'val_accuracy' in history.history else 0\n",
    "        else:\n",
    "            final_train_acc = final_val_acc = final_train_loss = final_val_loss = best_val_acc = 'N/A'\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': name,\n",
    "            'Total Parameters': f'{total_params:,}',\n",
    "            'Trainable Parameters': f'{trainable_params:,}',\n",
    "            'Final Train Accuracy': f'{final_train_acc:.4f}' if isinstance(final_train_acc, float) else final_train_acc,\n",
    "            'Final Val Accuracy': f'{final_val_acc:.4f}' if isinstance(final_val_acc, float) else final_val_acc,\n",
    "            'Best Val Accuracy': f'{best_val_acc:.4f}' if isinstance(best_val_acc, float) else best_val_acc,\n",
    "            'Final Train Loss': f'{final_train_loss:.4f}' if isinstance(final_train_loss, float) else final_train_loss,\n",
    "            'Final Val Loss': f'{final_val_loss:.4f}' if isinstance(final_val_loss, float) else final_val_loss\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create and display summary\n",
    "summary_df = create_performance_summary(models, training_histories)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv('../outputs/model_comparison_summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Summary saved to outputs/model_comparison_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architectures\n",
    "def visualize_model_architecture(model, model_name):\n",
    "    \"\"\"Create a visualization of the model architecture.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create model plot\n",
    "        tf.keras.utils.plot_model(\n",
    "            model,\n",
    "            to_file=f'../outputs/{model_name.lower().replace(\" \", \"_\")}_architecture.png',\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir='TB',\n",
    "            expand_nested=True,\n",
    "            dpi=150\n",
    "        )\n",
    "        print(f\"‚úÖ {model_name} architecture saved to outputs/\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save {model_name} architecture: {e}\")\n",
    "\n",
    "# Create architecture visualizations\n",
    "print(\"Creating model architecture visualizations...\")\n",
    "for name, model in models.items():\n",
    "    visualize_model_architecture(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EXPERIMENT CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç ARCHITECTURE ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for name, model in models.items():\n",
    "    params = model.count_params()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  ‚Ä¢ Parameters: {params:,}\")\n",
    "    print(f\"  ‚Ä¢ Model size: ~{params * 4 / (1024*1024):.2f} MB (float32)\")\n",
    "    \n",
    "    if 'Basic' in name:\n",
    "        print(f\"  ‚Ä¢ Characteristics: Simple CNN, good baseline\")\n",
    "    elif 'Improved' in name:\n",
    "        print(f\"  ‚Ä¢ Characteristics: ResNet blocks, better feature learning\")\n",
    "    elif 'Lightweight' in name:\n",
    "        print(f\"  ‚Ä¢ Characteristics: Separable convolutions, mobile-friendly\")\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = None\n",
    "best_val_acc = 0\n",
    "\n",
    "for name, history in training_histories.items():\n",
    "    if history is not None and 'val_accuracy' in history.history:\n",
    "        max_val_acc = max(history.history['val_accuracy'])\n",
    "        if max_val_acc > best_val_acc:\n",
    "            best_val_acc = max_val_acc\n",
    "            best_model = name\n",
    "\n",
    "if best_model:\n",
    "    print(f\"\\nüèÜ Best performing model: {best_model}\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No successful training runs to compare\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. For production deployment: Use Lightweight CNN for mobile/edge devices\")\n",
    "print(\"2. For maximum accuracy: Use Improved CNN with longer training\")\n",
    "print(\"3. For quick prototyping: Use Basic CNN as baseline\")\n",
    "print(\"4. Consider ensemble methods combining multiple architectures\")\n",
    "print(\"5. Implement transfer learning from pre-trained models for better performance\")\n",
    "\n",
    "print(\"\\nüîÑ NEXT STEPS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Run hyperparameter tuning on the best performing architecture\")\n",
    "print(\"2. Implement data augmentation strategies\")\n",
    "print(\"3. Try transfer learning with pre-trained models\")\n",
    "print(\"4. Evaluate models on test set for final performance metrics\")\n",
    "print(\"5. Optimize models for deployment (quantization, pruning)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}