{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreshHarvest Hyperparameter Tuning\n",
    "\n",
    "This notebook demonstrates hyperparameter tuning for the FreshHarvest fruit freshness classification model using various optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna not available. Install with: pip install optuna\n",
      "SciKeras not available. Install with: pip install scikeras\n",
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameter optimization libraries\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Optuna not available. Install with: pip install optuna\")\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    from scikeras.wrappers import KerasClassifier\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SciKeras not available. Install with: pip install scikeras\")\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "from cvProject_FreshHarvest.utils.common import read_yaml, setup_logging\n",
    "from cvProject_FreshHarvest.models.cnn_models import FreshHarvestCNN\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "- Image size: [224, 224]\n",
      "- Number of classes: 16\n",
      "- Batch size: 32\n",
      "- Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = read_yaml('../config/config.yaml')\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Image size: {config['data']['image_size']}\")\n",
    "print(f\"- Number of classes: {config['data']['num_classes']}\")\n",
    "print(f\"- Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"- Learning rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11199 images belonging to 16 classes.\n",
      "Found 3200 images belonging to 16 classes.\n",
      "Training samples: 11199\n",
      "Validation samples: 3200\n",
      "Classes: ['F_Banana', 'F_Lemon', 'F_Lulo', 'F_Mango', 'F_Orange', 'F_Strawberry', 'F_Tamarillo', 'F_Tomato', 'S_Banana', 'S_Lemon', 'S_Lulo', 'S_Mango', 'S_Orange', 'S_Strawberry', 'S_Tamarillo', 'S_Tomato']\n"
     ]
    }
   ],
   "source": [
    "# Create data generators\n",
    "def create_data_generators(batch_size=32, validation_split=0.2):\n",
    "    \"\"\"Create training and validation data generators.\"\"\"\n",
    "    \n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation data generator (no augmentation)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        '../data/processed/train',\n",
    "        target_size=tuple(config['data']['image_size']),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        '../data/processed/val',\n",
    "        target_size=tuple(config['data']['image_size']),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Create initial generators\n",
    "train_gen, val_gen = create_data_generators()\n",
    "print(f\"Training samples: {train_gen.samples}\")\n",
    "print(f\"Validation samples: {val_gen.samples}\")\n",
    "print(f\"Classes: {list(train_gen.class_indices.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter search space defined:\n",
      "- model_type: ['basic', 'improved', 'lightweight']\n",
      "- learning_rate: [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
      "- batch_size: [16, 32, 64]\n",
      "- optimizer: ['adam', 'rmsprop', 'sgd']\n",
      "- dropout_rate: [0.2, 0.3, 0.4, 0.5]\n",
      "- l2_regularization: [0.0001, 0.001, 0.01]\n",
      "- rotation_range: [10, 20, 30]\n",
      "- zoom_range: [0.05, 0.1, 0.15]\n",
      "- brightness_range: [0.1, 0.2, 0.3]\n",
      "- dense_units: [64, 128, 256]\n",
      "- conv_filters: [32, 64, 128]\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter search space\n",
    "HYPERPARAMETER_SPACE = {\n",
    "    # Model architecture\n",
    "    'model_type': ['basic', 'improved', 'lightweight'],\n",
    "    \n",
    "    # Training parameters\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "    \n",
    "    # Regularization\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'l2_regularization': [0.0001, 0.001, 0.01],\n",
    "    \n",
    "    # Data augmentation\n",
    "    'rotation_range': [10, 20, 30],\n",
    "    'zoom_range': [0.05, 0.1, 0.15],\n",
    "    'brightness_range': [0.1, 0.2, 0.3],\n",
    "    \n",
    "    # Architecture specific\n",
    "    'dense_units': [64, 128, 256],\n",
    "    'conv_filters': [32, 64, 128]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space defined:\")\n",
    "for param, values in HYPERPARAMETER_SPACE.items():\n",
    "    print(f\"- {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 07:50:20,833 - root - INFO - CNN model initialized\n",
      "2025-06-19 07:50:21,262 - root - INFO - Created lightweight CNN model with 83019 parameters\n",
      "Test model created with 83019 parameters\n"
     ]
    }
   ],
   "source": [
    "def create_model_with_params(learning_rate=0.001, dropout_rate=0.3, dense_units=128, model_type='lightweight'):\n",
    "    \"\"\"Create model with specified hyperparameters.\"\"\"\n",
    "    \n",
    "    # Initialize CNN model\n",
    "    cnn_model = FreshHarvestCNN('../config/config.yaml')\n",
    "    \n",
    "    # Create model based on type\n",
    "    if model_type == 'basic':\n",
    "        model = cnn_model.create_basic_cnn()\n",
    "    elif model_type == 'improved':\n",
    "        model = cnn_model.create_improved_cnn()\n",
    "    else:\n",
    "        model = cnn_model.create_lightweight_cnn()\n",
    "    \n",
    "    # Compile with specified parameters\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_model_with_params()\n",
    "print(f\"Test model created with {test_model.count_params()} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual grid search with 5 combinations...\n"
     ]
    }
   ],
   "source": [
    "def manual_grid_search(param_combinations, max_trials=5):\n",
    "    \"\"\"Perform manual grid search over hyperparameters.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(param_combinations[:max_trials]):\n",
    "        print(f\"\\nTrial {i+1}/{min(len(param_combinations), max_trials)}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = create_model_with_params(**params)\n",
    "            \n",
    "            # Create data generators with batch size\n",
    "            batch_size = params.get('batch_size', 32)\n",
    "            train_gen, val_gen = create_data_generators(batch_size=batch_size)\n",
    "            \n",
    "            # Create callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
    "            ]\n",
    "            \n",
    "            # Train model (short training for hyperparameter search)\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=5,  # Short training for hyperparameter search\n",
    "                validation_data=val_gen,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1,\n",
    "                steps_per_epoch=min(50, train_gen.samples // batch_size),\n",
    "                validation_steps=min(20, val_gen.samples // batch_size)\n",
    "            )\n",
    "            \n",
    "            # Record results\n",
    "            best_val_acc = max(history.history['val_accuracy'])\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            result = {\n",
    "                'trial': i + 1,\n",
    "                'params': params,\n",
    "                'val_accuracy': best_val_acc,\n",
    "                'val_loss': best_val_loss,\n",
    "                'model_params': model.count_params()\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define a few parameter combinations to test\n",
    "test_combinations = [\n",
    "    {'learning_rate': 0.001, 'batch_size': 32, 'model_type': 'lightweight'},\n",
    "    {'learning_rate': 0.0005, 'batch_size': 32, 'model_type': 'lightweight'},\n",
    "    {'learning_rate': 0.001, 'batch_size': 16, 'model_type': 'lightweight'},\n",
    "    {'learning_rate': 0.001, 'batch_size': 32, 'model_type': 'basic'},\n",
    "    {'learning_rate': 0.005, 'batch_size': 32, 'model_type': 'lightweight'}\n",
    "]\n",
    "\n",
    "print(f\"Starting manual grid search with {len(test_combinations)} combinations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Manual Grid Search...\n",
      "\n",
      "Trial 1/3\n",
      "Parameters: {'learning_rate': 0.001, 'batch_size': 32, 'model_type': 'lightweight'}\n",
      "Trial failed: create_model_with_params() got an unexpected keyword argument 'batch_size'\n",
      "\n",
      "Trial 2/3\n",
      "Parameters: {'learning_rate': 0.0005, 'batch_size': 32, 'model_type': 'lightweight'}\n",
      "Trial failed: create_model_with_params() got an unexpected keyword argument 'batch_size'\n",
      "\n",
      "Trial 3/3\n",
      "Parameters: {'learning_rate': 0.001, 'batch_size': 16, 'model_type': 'lightweight'}\n",
      "Trial failed: create_model_with_params() got an unexpected keyword argument 'batch_size'\n",
      "❌ No successful trials in grid search\n"
     ]
    }
   ],
   "source": [
    "# Run manual grid search\n",
    "if len(test_combinations) > 0:\n",
    "    print(\"\\n🚀 Starting Manual Grid Search...\")\n",
    "    results = manual_grid_search(test_combinations, max_trials=3)\n",
    "    \n",
    "    if results:\n",
    "        # Convert to DataFrame for analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\n📊 Grid Search Results:\")\n",
    "        print(results_df[['trial', 'val_accuracy', 'val_loss', 'model_params']])\n",
    "        \n",
    "        # Find best configuration\n",
    "        best_idx = results_df['val_accuracy'].idxmax()\n",
    "        best_result = results_df.iloc[best_idx]\n",
    "        \n",
    "        print(f\"\\n🏆 Best Configuration:\")\n",
    "        print(f\"   Trial: {best_result['trial']}\")\n",
    "        print(f\"   Validation Accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "        print(f\"   Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "        print(f\"   Parameters: {best_result['params']}\")\n",
    "    else:\n",
    "        print(\"❌ No successful trials in grid search\")\n",
    "else:\n",
    "    print(\"⚠️ No test combinations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Optuna not available. Install with: pip install optuna\n"
     ]
    }
   ],
   "source": [
    "# Optuna optimization (if available)\n",
    "if OPTUNA_AVAILABLE:\n",
    "    print(\"🔬 Setting up Optuna optimization...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function.\"\"\"\n",
    "        \n",
    "        # Suggest hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        model_type = trial.suggest_categorical('model_type', ['basic', 'lightweight'])\n",
    "        \n",
    "        try:\n",
    "            # Create model with suggested parameters\n",
    "            model = create_model_with_params(\n",
    "                learning_rate=learning_rate,\n",
    "                dropout_rate=dropout_rate,\n",
    "                model_type=model_type\n",
    "            )\n",
    "            \n",
    "            # Create data generators\n",
    "            train_gen, val_gen = create_data_generators(batch_size=batch_size)\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=3,  # Short training for optimization\n",
    "                validation_data=val_gen,\n",
    "                verbose=0,\n",
    "                steps_per_epoch=min(20, train_gen.samples // batch_size),\n",
    "                validation_steps=min(10, val_gen.samples // batch_size)\n",
    "            )\n",
    "            \n",
    "            # Return best validation accuracy\n",
    "            return max(history.history['val_accuracy'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    # Create study and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5, timeout=1800)  # 30 minutes max\n",
    "    \n",
    "    print(\"\\n🏆 Optuna Optimization Results:\")\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best value: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "    \n",
    "    # Plot optimization history\n",
    "    try:\n",
    "        fig = optuna.visualization.plot_optimization_history(study)\n",
    "        fig.show()\n",
    "        \n",
    "        fig = optuna.visualization.plot_param_importances(study)\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Optuna not available. Install with: pip install optuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hyperparameter tuning results to analyze\n"
     ]
    }
   ],
   "source": [
    "# Analyze hyperparameter importance\n",
    "def analyze_hyperparameter_importance(results):\n",
    "    \"\"\"Analyze which hyperparameters have the most impact.\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Extract hyperparameters and performance\n",
    "    param_data = []\n",
    "    for result in results:\n",
    "        params = result['params'].copy()\n",
    "        params['val_accuracy'] = result['val_accuracy']\n",
    "        param_data.append(params)\n",
    "    \n",
    "    df = pd.DataFrame(param_data)\n",
    "    \n",
    "    # Correlation analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Hyperparameter Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Performance by categorical parameters\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        fig, axes = plt.subplots(1, len(categorical_cols), figsize=(5*len(categorical_cols), 5))\n",
    "        if len(categorical_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            if col != 'val_accuracy':\n",
    "                df.boxplot(column='val_accuracy', by=col, ax=axes[i])\n",
    "                axes[i].set_title(f'Validation Accuracy by {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze results if available\n",
    "if 'results' in locals() and results:\n",
    "    analyze_hyperparameter_importance(results)\n",
    "else:\n",
    "    print(\"No hyperparameter tuning results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Configuration Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No best configuration found to train final model\n"
     ]
    }
   ],
   "source": [
    "# Train final model with best configuration\n",
    "def train_best_model(best_params, epochs=20):\n",
    "    \"\"\"Train model with best hyperparameters for longer.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Training final model with best parameters: {best_params}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    model = create_model_with_params(**best_params)\n",
    "    \n",
    "    # Create data generators\n",
    "    batch_size = best_params.get('batch_size', 32)\n",
    "    train_gen, val_gen = create_data_generators(batch_size=batch_size)\n",
    "    \n",
    "    # Enhanced callbacks for final training\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath='../models/best_hypertuned_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train best model if we have results\n",
    "if 'best_result' in locals():\n",
    "    best_model, best_history = train_best_model(best_result['params'])\n",
    "    \n",
    "    # Plot final training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(best_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(best_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✅ Best model saved to models/best_hypertuned_model.h5\")\n",
    "    print(f\"Final validation accuracy: {max(best_history.history['val_accuracy']):.4f}\")\n",
    "else:\n",
    "    print(\"No best configuration found to train final model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "🔍 SEARCH SPACE EXPLORED:\n",
      "  • model_type: ['basic', 'improved', 'lightweight']\n",
      "  • learning_rate: [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
      "  • batch_size: [16, 32, 64]\n",
      "  • optimizer: ['adam', 'rmsprop', 'sgd']\n",
      "  • dropout_rate: [0.2, 0.3, 0.4, 0.5]\n",
      "  • l2_regularization: [0.0001, 0.001, 0.01]\n",
      "  • rotation_range: [10, 20, 30]\n",
      "  • zoom_range: [0.05, 0.1, 0.15]\n",
      "  • brightness_range: [0.1, 0.2, 0.3]\n",
      "  • dense_units: [64, 128, 256]\n",
      "  • conv_filters: [32, 64, 128]\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "  1. Run longer training with best hyperparameters\n",
      "  2. Implement advanced techniques (learning rate scheduling, warm restarts)\n",
      "  3. Try ensemble methods with top configurations\n",
      "  4. Explore transfer learning with pre-trained models\n",
      "  5. Optimize for deployment (quantization, pruning)\n",
      "\n",
      "📝 RECOMMENDATIONS:\n",
      "  • Use automated hyperparameter optimization for production models\n",
      "  • Consider multi-objective optimization (accuracy vs speed vs size)\n",
      "  • Implement cross-validation for more robust hyperparameter selection\n",
      "  • Monitor hyperparameter sensitivity for model stability\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔍 SEARCH SPACE EXPLORED:\")\n",
    "for param, values in HYPERPARAMETER_SPACE.items():\n",
    "    print(f\"  • {param}: {values}\")\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"\\n📊 TRIALS COMPLETED: {len(results)}\")\n",
    "    \n",
    "    if 'best_result' in locals():\n",
    "        print(f\"\\n🏆 BEST CONFIGURATION:\")\n",
    "        print(f\"  • Validation Accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "        print(f\"  • Parameters: {best_result['params']}\")\n",
    "        \n",
    "        print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "        best_params = best_result['params']\n",
    "        \n",
    "        if 'learning_rate' in best_params:\n",
    "            lr = best_params['learning_rate']\n",
    "            if lr < 0.001:\n",
    "                print(f\"  • Lower learning rate ({lr}) performed best - suggests need for careful optimization\")\n",
    "            elif lr > 0.005:\n",
    "                print(f\"  • Higher learning rate ({lr}) performed best - model can handle aggressive training\")\n",
    "            else:\n",
    "                print(f\"  • Moderate learning rate ({lr}) performed best - balanced approach\")\n",
    "        \n",
    "        if 'model_type' in best_params:\n",
    "            model_type = best_params['model_type']\n",
    "            print(f\"  • {model_type.capitalize()} architecture performed best\")\n",
    "        \n",
    "        if 'batch_size' in best_params:\n",
    "            batch_size = best_params['batch_size']\n",
    "            if batch_size <= 16:\n",
    "                print(f\"  • Smaller batch size ({batch_size}) preferred - suggests noisy gradients help\")\n",
    "            else:\n",
    "                print(f\"  • Larger batch size ({batch_size}) preferred - stable gradient updates\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"  1. Run longer training with best hyperparameters\")\n",
    "print(\"  2. Implement advanced techniques (learning rate scheduling, warm restarts)\")\n",
    "print(\"  3. Try ensemble methods with top configurations\")\n",
    "print(\"  4. Explore transfer learning with pre-trained models\")\n",
    "print(\"  5. Optimize for deployment (quantization, pruning)\")\n",
    "\n",
    "print(\"\\n📝 RECOMMENDATIONS:\")\n",
    "print(\"  • Use automated hyperparameter optimization for production models\")\n",
    "print(\"  • Consider multi-objective optimization (accuracy vs speed vs size)\")\n",
    "print(\"  • Implement cross-validation for more robust hyperparameter selection\")\n",
    "print(\"  • Monitor hyperparameter sensitivity for model stability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
