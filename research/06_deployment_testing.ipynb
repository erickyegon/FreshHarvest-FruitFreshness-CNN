{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreshHarvest Deployment Testing\n",
    "\n",
    "This notebook provides comprehensive testing for FreshHarvest model deployment including:\n",
    "- Model loading and inference testing\n",
    "- Performance benchmarking\n",
    "- API endpoint testing\n",
    "- Streamlit application testing\n",
    "- Docker deployment validation\n",
    "- Production readiness checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n",
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Import custom modules\n",
    "from cvProject_FreshHarvest.utils.common import read_yaml, setup_logging\n",
    "from cvProject_FreshHarvest.models.cnn_models import FreshHarvestCNN\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing Model Loading...\n",
      "==================================================\n",
      "‚ö†Ô∏è ../models/trained/best_model.h5: File not found\n",
      "‚ö†Ô∏è ../models/best_hypertuned_model.h5: File not found\n",
      "2025-06-19 07:54:13,775 - absl - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "‚úÖ ../models/checkpoints/best_model_20250618_100126.h5\n",
      "   Load time: 0.411s\n",
      "   Model size: 1.04 MB\n",
      "   Parameters: 83,019\n"
     ]
    }
   ],
   "source": [
    "# Test model loading from different sources\n",
    "def test_model_loading():\n",
    "    \"\"\"Test loading models from different file formats and locations.\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Testing Model Loading...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different model paths\n",
    "    model_paths = [\n",
    "        '../models/trained/best_model.h5',\n",
    "        '../models/best_hypertuned_model.h5',\n",
    "        '../models/checkpoints/best_model_20250618_100126.h5'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for path in model_paths:\n",
    "        result = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'loadable': False,\n",
    "            'load_time': None,\n",
    "            'model_size': None,\n",
    "            'parameters': None\n",
    "        }\n",
    "        \n",
    "        if result['exists']:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                model = keras.models.load_model(path)\n",
    "                load_time = time.time() - start_time\n",
    "                \n",
    "                result['loadable'] = True\n",
    "                result['load_time'] = load_time\n",
    "                result['model_size'] = os.path.getsize(path) / (1024 * 1024)  # MB\n",
    "                result['parameters'] = model.count_params()\n",
    "                \n",
    "                print(f\"‚úÖ {path}\")\n",
    "                print(f\"   Load time: {load_time:.3f}s\")\n",
    "                print(f\"   Model size: {result['model_size']:.2f} MB\")\n",
    "                print(f\"   Parameters: {result['parameters']:,}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {path}: File not found\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run model loading tests\n",
    "loading_results = test_model_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-19 07:54:14,311 - absl - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n",
      "Using model: ../models/checkpoints/best_model_20250618_100126.h5\n",
      "\n",
      "‚ö° Testing Inference Performance (100 iterations)...\n",
      "============================================================\n",
      "Single Image Inference:\n",
      "  Mean time: 0.1830s\n",
      "  Std time: 0.0619s\n",
      "  Min time: 0.1274s\n",
      "  Max time: 0.4213s\n",
      "\n",
      "Batch Inference Results:\n",
      "  Batch size  1: 0.2033s/image, 4.92 images/s\n",
      "  Batch size  8: 0.0297s/image, 33.69 images/s\n",
      "  Batch size 16: 0.0229s/image, 43.76 images/s\n",
      "  Batch size 32: 0.0132s/image, 76.03 images/s\n"
     ]
    }
   ],
   "source": [
    "# Test inference performance\n",
    "def test_inference_performance(model, num_tests=100):\n",
    "    \"\"\"Test model inference performance.\"\"\"\n",
    "    \n",
    "    print(f\"\\n‚ö° Testing Inference Performance ({num_tests} iterations)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create dummy test data\n",
    "    test_images = np.random.random((num_tests, 224, 224, 3))\n",
    "    \n",
    "    # Warm up the model\n",
    "    _ = model.predict(test_images[:5], verbose=0)\n",
    "    \n",
    "    # Test single image inference\n",
    "    single_times = []\n",
    "    for i in range(min(20, num_tests)):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(test_images[i:i+1], verbose=0)\n",
    "        inference_time = time.time() - start_time\n",
    "        single_times.append(inference_time)\n",
    "    \n",
    "    # Test batch inference\n",
    "    batch_sizes = [1, 8, 16, 32]\n",
    "    batch_results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size <= num_tests:\n",
    "            start_time = time.time()\n",
    "            _ = model.predict(test_images[:batch_size], verbose=0)\n",
    "            total_time = time.time() - start_time\n",
    "            avg_time_per_image = total_time / batch_size\n",
    "            \n",
    "            batch_results.append({\n",
    "                'batch_size': batch_size,\n",
    "                'total_time': total_time,\n",
    "                'avg_time_per_image': avg_time_per_image,\n",
    "                'throughput': batch_size / total_time\n",
    "            })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Single Image Inference:\")\n",
    "    print(f\"  Mean time: {np.mean(single_times):.4f}s\")\n",
    "    print(f\"  Std time: {np.std(single_times):.4f}s\")\n",
    "    print(f\"  Min time: {np.min(single_times):.4f}s\")\n",
    "    print(f\"  Max time: {np.max(single_times):.4f}s\")\n",
    "    \n",
    "    print(f\"\\nBatch Inference Results:\")\n",
    "    for result in batch_results:\n",
    "        print(f\"  Batch size {result['batch_size']:2d}: {result['avg_time_per_image']:.4f}s/image, {result['throughput']:.2f} images/s\")\n",
    "    \n",
    "    return single_times, batch_results\n",
    "\n",
    "# Load a model for testing\n",
    "test_model = None\n",
    "for result in loading_results:\n",
    "    if result['loadable']:\n",
    "        test_model = keras.models.load_model(result['path'])\n",
    "        print(f\"\\nUsing model: {result['path']}\")\n",
    "        break\n",
    "\n",
    "if test_model is None:\n",
    "    print(\"\\n‚ö†Ô∏è No trained model available. Creating lightweight model for testing.\")\n",
    "    config = read_yaml('../config/config.yaml')\n",
    "    cnn_builder = FreshHarvestCNN('../config/config.yaml')\n",
    "    test_model = cnn_builder.create_lightweight_cnn()\n",
    "    test_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Run performance tests\n",
    "single_times, batch_results = test_inference_performance(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Usage Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Testing Memory Usage...\n",
      "========================================\n",
      "Initial memory usage: 869.22 MB\n",
      "Memory after model loading: 869.23 MB\n",
      "Model memory overhead: 0.01 MB\n",
      "Memory during inference: 865.73 MB\n",
      "Inference memory overhead: -3.50 MB\n",
      "\n",
      "Model efficiency:\n",
      "  Theoretical model size: 0.32 MB\n",
      "  Actual memory usage: 0.01 MB\n",
      "  Memory efficiency: 4053.7%\n"
     ]
    }
   ],
   "source": [
    "# Test memory usage\n",
    "def test_memory_usage():\n",
    "    \"\"\"Test memory usage during model operations.\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Testing Memory Usage...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        \n",
    "        # Initial memory\n",
    "        initial_memory = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "        print(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "        \n",
    "        # Memory after model loading\n",
    "        if test_model is not None:\n",
    "            model_memory = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "            print(f\"Memory after model loading: {model_memory:.2f} MB\")\n",
    "            print(f\"Model memory overhead: {model_memory - initial_memory:.2f} MB\")\n",
    "            \n",
    "            # Memory during inference\n",
    "            test_batch = np.random.random((32, 224, 224, 3))\n",
    "            _ = test_model.predict(test_batch, verbose=0)\n",
    "            inference_memory = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "            print(f\"Memory during inference: {inference_memory:.2f} MB\")\n",
    "            print(f\"Inference memory overhead: {inference_memory - model_memory:.2f} MB\")\n",
    "            \n",
    "            # Memory efficiency\n",
    "            model_params = test_model.count_params()\n",
    "            theoretical_size = model_params * 4 / (1024 * 1024)  # Assuming float32\n",
    "            print(f\"\\nModel efficiency:\")\n",
    "            print(f\"  Theoretical model size: {theoretical_size:.2f} MB\")\n",
    "            print(f\"  Actual memory usage: {model_memory - initial_memory:.2f} MB\")\n",
    "            print(f\"  Memory efficiency: {theoretical_size / (model_memory - initial_memory) * 100:.1f}%\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è psutil not available. Install with: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Memory testing failed: {e}\")\n",
    "\n",
    "# Run memory usage tests\n",
    "test_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streamlit Application Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Testing Streamlit Application...\n",
      "=============================================\n",
      "‚úÖ Found Streamlit app: ../app_simple.py\n",
      "   ‚ùå Error checking file: 'charmap' codec can't decode byte 0x8d in position 756: character maps to <undefined>\n",
      "‚úÖ Found Streamlit app: ../app.py\n",
      "   ‚ùå Error checking file: 'charmap' codec can't decode byte 0x8d in position 937: character maps to <undefined>\n",
      "\n",
      "‚úÖ Streamlit version: 1.45.1\n"
     ]
    }
   ],
   "source": [
    "# Test Streamlit application\n",
    "def test_streamlit_app():\n",
    "    \"\"\"Test if Streamlit application can be started.\"\"\"\n",
    "    \n",
    "    print(\"\\nüåê Testing Streamlit Application...\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Check if Streamlit files exist\n",
    "    streamlit_files = [\n",
    "        '../app_simple.py',\n",
    "        '../app.py'\n",
    "    ]\n",
    "    \n",
    "    for app_file in streamlit_files:\n",
    "        if os.path.exists(app_file):\n",
    "            print(f\"‚úÖ Found Streamlit app: {app_file}\")\n",
    "            \n",
    "            # Check if the file is valid Python\n",
    "            try:\n",
    "                with open(app_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                # Basic syntax check\n",
    "                compile(content, app_file, 'exec')\n",
    "                print(f\"   ‚úÖ Syntax check passed\")\n",
    "                \n",
    "                # Check for required imports\n",
    "                required_imports = ['streamlit', 'tensorflow', 'numpy']\n",
    "                for imp in required_imports:\n",
    "                    if imp in content:\n",
    "                        print(f\"   ‚úÖ {imp} import found\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è {imp} import not found\")\n",
    "                        \n",
    "            except SyntaxError as e:\n",
    "                print(f\"   ‚ùå Syntax error: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error checking file: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Streamlit app not found: {app_file}\")\n",
    "    \n",
    "    # Test if Streamlit can be imported\n",
    "    try:\n",
    "        import streamlit as st\n",
    "        print(f\"\\n‚úÖ Streamlit version: {st.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"\\n‚ùå Streamlit not installed. Install with: pip install streamlit\")\n",
    "\n",
    "# Run Streamlit tests\n",
    "test_streamlit_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
