{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreshHarvest Performance Monitoring\n",
    "\n",
    "This notebook provides comprehensive performance monitoring and analysis for FreshHarvest models.\n",
    "\n",
    "## Monitoring Overview\n",
    "- Model inference performance\n",
    "- Memory usage analysis\n",
    "- Batch processing benchmarks\n",
    "- Real-time prediction monitoring\n",
    "- Resource utilization tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import custom modules\n",
    "from cvProject_FreshHarvest.utils.common import read_yaml, setup_logging\n",
    "\n",
    "# Setup\n",
    "setup_logging()\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"FreshHarvest Performance Monitoring Notebook\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"System: {psutil.cpu_count()} CPUs, {psutil.virtual_memory().total / (1024**3):.1f} GB RAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = read_yaml('../config/config.yaml')\n",
    "\n",
    "# Load trained model\n",
    "model_paths = [\n",
    "    '../models/trained/best_model.h5',\n",
    "    '../models/checkpoints/best_model_20250618_100126.h5'\n",
    "]\n",
    "\n",
    "model = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            model = keras.models.load_model(path)\n",
    "            print(f\"‚úÖ Model loaded from: {path}\")\n",
    "            print(f\"Model parameters: {model.count_params():,}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {path}: {e}\")\n",
    "\n",
    "if model is None:\n",
    "    print(\"‚ö†Ô∏è No model found. Creating lightweight model for demonstration.\")\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(224, 224, 3)),\n",
    "        keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(16, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Input shape: {model.input_shape}\")\n",
    "print(f\"Output shape: {model.output_shape}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, batch_sizes=[1, 4, 8, 16, 32], num_iterations=10):\n",
    "    \"\"\"Benchmark model inference performance.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"üöÄ Running inference benchmarks...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Testing batch size: {batch_size}\")\n",
    "        \n",
    "        # Create dummy data\n",
    "        dummy_data = np.random.random((batch_size, 224, 224, 3))\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(3):\n",
    "            _ = model.predict(dummy_data, verbose=0)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        memory_usage = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # Memory before\n",
    "            mem_before = psutil.virtual_memory().used / (1024**2)  # MB\n",
    "            \n",
    "            # Time inference\n",
    "            start_time = time.time()\n",
    "            predictions = model.predict(dummy_data, verbose=0)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Memory after\n",
    "            mem_after = psutil.virtual_memory().used / (1024**2)  # MB\n",
    "            \n",
    "            inference_time = end_time - start_time\n",
    "            times.append(inference_time)\n",
    "            memory_usage.append(mem_after - mem_before)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        avg_memory = np.mean(memory_usage)\n",
    "        throughput = batch_size / avg_time  # images per second\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'avg_memory': avg_memory,\n",
    "            'throughput': throughput,\n",
    "            'time_per_image': avg_time / batch_size\n",
    "        })\n",
    "        \n",
    "        print(f\"  Avg time: {avg_time:.4f}s ¬± {std_time:.4f}s\")\n",
    "        print(f\"  Throughput: {throughput:.2f} images/sec\")\n",
    "        print(f\"  Memory: {avg_memory:.2f} MB\")\n",
    "        print()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_inference(model)\n",
    "print(\"üìä Benchmark Results:\")\n",
    "print(benchmark_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Inference time vs batch size\n",
    "axes[0, 0].plot(benchmark_results['batch_size'], benchmark_results['avg_time'], 'bo-')\n",
    "axes[0, 0].fill_between(benchmark_results['batch_size'], \n",
    "                       benchmark_results['avg_time'] - benchmark_results['std_time'],\n",
    "                       benchmark_results['avg_time'] + benchmark_results['std_time'],\n",
    "                       alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Batch Size')\n",
    "axes[0, 0].set_ylabel('Inference Time (seconds)')\n",
    "axes[0, 0].set_title('Inference Time vs Batch Size')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Throughput vs batch size\n",
    "axes[0, 1].plot(benchmark_results['batch_size'], benchmark_results['throughput'], 'go-')\n",
    "axes[0, 1].set_xlabel('Batch Size')\n",
    "axes[0, 1].set_ylabel('Throughput (images/sec)')\n",
    "axes[0, 1].set_title('Throughput vs Batch Size')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time per image vs batch size\n",
    "axes[1, 0].plot(benchmark_results['batch_size'], benchmark_results['time_per_image'] * 1000, 'ro-')\n",
    "axes[1, 0].set_xlabel('Batch Size')\n",
    "axes[1, 0].set_ylabel('Time per Image (ms)')\n",
    "axes[1, 0].set_title('Time per Image vs Batch Size')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Memory usage vs batch size\n",
    "axes[1, 1].plot(benchmark_results['batch_size'], benchmark_results['avg_memory'], 'mo-')\n",
    "axes[1, 1].set_xlabel('Batch Size')\n",
    "axes[1, 1].set_ylabel('Memory Usage (MB)')\n",
    "axes[1, 1].set_title('Memory Usage vs Batch Size')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "print(f\"Best throughput: {benchmark_results['throughput'].max():.2f} images/sec at batch size {benchmark_results.loc[benchmark_results['throughput'].idxmax(), 'batch_size']}\")\n",
    "print(f\"Fastest single image: {benchmark_results['time_per_image'].min()*1000:.2f} ms at batch size {benchmark_results.loc[benchmark_results['time_per_image'].idxmin(), 'batch_size']}\")\n",
    "print(f\"Lowest memory usage: {benchmark_results['avg_memory'].min():.2f} MB at batch size {benchmark_results.loc[benchmark_results['avg_memory'].idxmin(), 'batch_size']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}